# tdr-data-retention-reviewer
This repository contains code for a scripted process that reviews published and unpublished datasets in a Dataverse instance and produces reports which identify datasets to be considered for deaccessioning.

It has been developed to specifically support data retention decision making in the Texas Data Repository (https://dataverse.tdl.org/) but designed to be adaptable for other Dataverse installations.


### Configuring the .env file
Once this repo is cloned locally, the .env.template file should be renamed to just .env and the contents of the file should be edited to replace the example values that are provided in the file by default with the correct values based on the institution for which the script will be run. Take care to preserve the JSON formatting of the .env file to ensure proper functioning of the Python scripts in the repository which depend on the parameters defined in the .env file.

#### Explanation of configurable parameters
* githubtoken:  The personalized GitHub token generated by the user of the script should be supplied here
* institutionname:  The standard institution name - this will be used to name files and manage other script processes
* institutionnamepermutations:  A list of all of the variations of the institution name that GitHub users might mention in their account bio statement
* institutioncity:  The city that the institution is located in
* institutionemaildomain:  The email address extension for the institution (e.g. "utexas.edu")
* githubaccountdetailscsvpath:  Path to CSV file containing GitHub account results
* githubrepodetailscsvpath:  Path to CSV file containing GitHub repository results
* resultsperpage:  Number of GitHub results per page to return
* pagelimit:  Number of pages of GitHub results to return
* minimumfollowers:  Minimum number of followers that a GitHub account must have for it to be included in results
* minimumrepos:  Minimum number of repositories that a GitHub account must have for it to be included in results
* githubrepolastupdatethresholdinmonths:  An integer used to restrict repository data gathered so that only repositories updated with the specified number of months will be included in the saved CSV. This can be helpful for filtering out projects that are no longer active.
* detaillevel:  "fulldetail" or "limiteddetail" - this controls whether the email, company, and bio fields will be filled in in the results CSV
* githubaccountdatacsvpathforvisualization:  The file path to the specific CSV that contains GitHub account data that was gathered by the script that should be used for generating data visualizations
* githubrepodatacsvpathforvisualization:  The file path to the specific CSV that contains GitHub repository data that was gathered by the script that should be used for generating data visualizations

* dataverse_api_key":"",
* dataverse_api_host":"https://dataverse.tdl.org",
* showdatasetdetails":"True",
* showdataretentionscoredetails":"True",
* institutionaldataverse":"utexas",
* unpublisheddatasetreviewthresholdinyears":1,
* publisheddatasetreviewthresholdinyears":2,
* unpublisheddatasetreviewthresholdingb":1,
* publisheddatasetreviewthresholdingb":2,
* mitigatingfactormincitationcount":1,
* mitigatingfactormindownloadcount":1,
* mitigatingfactorfundedresearch":"True",
* processunpublisheddatasets":"True",
* processpublisheddatasets":"True",
* processdeaccessioneddatasets":"True"

* dataverse_api_key:  "", The personalized GitHub token generated by the user of the script should be supplied
* dataverse_api_host: "", The base URL of the dataverse instance, e.g. "https://dataverse.tdl.org"
* showdatasetdetails: "True" or "False", default = "True", determines if dataset metadata should be recorded in log files
* showdataretentionscoredetails: "True" or "False", default = "True", determines if data retention score details should be included in generated reports
* institutionaldataverse": Name of individual dataverse instance within multi-instutional Dataverse installation, e.g. "utexas" could be used to identify UT Austin datasets within the Texas Data Repository
* unpublisheddatasetreviewthresholdinyears":1, the threshold for determining how long a dataset can remain unpublished in a Dataverse instance before being identified as needing review for potential deaccessioning - all unpublished datasets less than this many years old will be listed as not needing review
* publisheddatasetreviewthresholdinyears":2, the threshold for determining how long a dataset can remain published in a Dataverse instance before being identified as needing review for potential deaccessioning - all published datasets less than this many years old will be listed as not needing review
* unpublisheddatasetreviewthresholdingb":1, the threshold for determining how large an unpublished dataset can be in a Dataverse instance before being identified as needing review for potential deaccessioning - all unpublished datasets less than this many GB will be listed as not needing review even if they exceed the age threshold for unpublished datasets defined above
* publisheddatasetreviewthresholdingb":2,the threshold for determining how large a published dataset can be in a Dataverse instance before being identified as needing review for potential deaccessioning - all published datasets less than this many GB will be listed as not needing review even if they exceed the age threshold for published datasets defined above
* mitigatingfactormincitationcount":1, the threshold for the minimum number of citations for a dataset which will be used to determine if a dataset exceeding the age threshold and size threshold should still be retained and not considered for deaccessioning
* mitigatingfactormindownloadcount":1, the threshold for the minimum number of downloads for a dataset which will be used to determine if a dataset exceeding the age threshold and size threshold should still be retained and not considered for deaccessioning
* mitigatingfactorfundedresearch: "True" or "False", default = "True" this binary value determines if a datasets which have associated grant funding information in their metadata should still be retained and not considered for deaccessioning even if they exceed the age and size threshold set above
* processunpublisheddatasets: "True" or "False", default = "True", determines if unpublished datasets should be processed by the script - this can be set to false if unpublished datasets will not be considered for removal
* processpublisheddatasets: "True" or "False", default = "True", determines if published datasets should be processed by the script - this can be set to false if published datasets will not be considered for removal
* processdeaccessioneddatasets: "True" or "False", default = "True", determines if deaccessioned datasets should be processed by the script - this can be set to false if you do not want the script to spend time gathering information about deaccessioned datasets

## Contact
For any questions about this repository, please contact the the UT Austin RDS team that has lead initial development of this tool by sending an email to utl-rds@austin.utexas.edu.